---
title: "Data Transformation in R with dplyr - Answers"
author: "Jennifer Huck"
date: "Spring 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Before we get started:

* Go to Tools -> Global Options -> Code -> Make sure "Soft-Wrap R Source Files" is checked.
* Go to Tools -> Global Options -> R Markdown -> Make sure "Show Output Inline for all R Markdown Documents" is checked.

# Introducing dplyr
We will be working with the `dplyr` package today.  `dplyr` is useful when working with tabular data to rename and select columns, and summarize insights from your data.  `dplyr` is part of the tidyverse - a larger collection of packages designed for data science.  

We will very briefly rely on `readr` package as well.

You might want to reference UVA Library's Michele Claibourn's [Data Exploration and Wrangling](https://uvastatlab.github.io/phdplus2021/wrangling_prep.html) page.  That offers a useful, quick overview of the main dplyr functions. 

The dplyr workshop materials are available on [GitHub](https://github.com/jennhuck/dplyr).

## Our data: Albemarle County Homes
We will be working with Albemarle County real estate data.  The original data came from the Albemarle County Office of Geographic Services.  The RDS team has already cleaned this data from its original, raw, messy form.  If you are interested in following the steps from raw to final form, check out the [PhD Plus series](https://uvastatlab.github.io/phdplus2020/) for Exploratory Data Analysis and Data Preparation.

## Quick note: R Markdown
We are going to run this session in R Markdown.  R Markdown is great because it contains both your code and the narration that a reader needs to understand your work, all in one file.  This also makes it MUCH easier for someone else to understand your work, or for you to understand your own work six months in the future.  Simply re-run the code in your R Markdown (.rmd) file, and export the result as html, pdf, or other file types.  

For this session, we will run code *chunks* by clicking on the green, right-pointing triangle arrow, or click into the code chunk and type *CTRL/CMD+SHIFT+Enter* to run the entire code chunk or *CTRL/CMD+Enter* to run a line or a highlighted section.  

You can optionally clear the output by clicking the X in the upper right corner.

We will get some red text after loading our packages -- that's fine.  It is just a regular message.

## Load libraries, read in dataset

Let's start by loading our packages:
```{r load packages}
library(dplyr) # "a consistent set of verbs that help you solve the most common data manipulation challenges"
library(readr) # "a fast and friendly way to read rectangular data"
```

Let's read in our data with the `readr::read_csv()` function.  We will also use `here::here()` for simple file management. The `here` package offers a simple way to work with your files, especially in an R Markdown setting.

Note from a file management perspective, we have our raw data in a folder designated specifically for raw data. 

```{r import data}
homes <- read_csv("../data-raw/albemarle_homes_2020.csv")
```

Let's inspect and preview our data.

```{r inspect and preview}
# inspect the data
homes

# alternative ways to preview the data
# View(homes)
# glimpse(homes)
# str(homes)
# head(homes)

```

## Rename columns
Let's tidy up our column names.  It is helpful to have simple, meaningful names, that are formatted consistently.  Sometimes you have to type column names out in R, so make it easy on yourself. 

Note that I will use "column" and "variable" interchangeably.  Later, I will also use "row" and "observation" interchangeably.

In R, a general rule is avoid spaces and dashes in your variable names, and start your variable names with a letter.

Let's review and rename column names.

```{r review names}
# review columns names
names(homes)
```

These names look good, except for that `High School District` - it is inconsistent with the format of the other district variables.  It has spaces which means you have to wrap it in quotes to call it, and that's annoying.  Let's rename it, and we will save the object this time.
```{r rename hsdistrict}
# rename problematic column name
# use this format: new_name = old_name
homes <- rename(homes, hsdistrict = "High School District")
names(homes)
```

## Select: select columns
To select a few columns in a data frame, use the `select()` function.  The first argument in the function is the data frame (`homes`), and everything following that are columns to keep.
```{r select simple}
select(homes, totalvalue, yearbuilt, finsqft) # select three variables
select(homes, lotsize:totalvalue) # select variables between lotsize and totval
select(homes, contains("year")) # contains a word
select(homes, ends_with("district")) # ends with a word
```

Here's a slightly more complex example: select the variables that I know we will be reviewing (because it's convenient to see them first in these displays) (e.g., totalvalue), add everything else back in, remove column `condition2`, and save to our `homes` object.

(`condition2` is one of the variables in our dataframe that we don't need.  It is a duplicate of the `condition` variable, that tells us if the home is in "excellent," "fair," etc. condition.)
```{r select complex}
homes <- select(homes, totalvalue, esdistrict, finsqft, halfbath, everything(), -condition2)
homes
```

## Your turn #1
Using the `homes` data, keep the `condition`, `cooling,` and `bedroom` variables only. (Do not update the data frame.)
```{r your_turn 1}
select(homes, condition, cooling, bedroom)
```

## Quick review of our data
First, let's review our data with the `summary()` function:
```{r summary}
# get a summary of dataset variables
summary(homes)
```

Here's a quick review of the school names in our dataset.  We can use the `distinct()` function to select only "distinct" or unique rows in a data frame.
```{r distinct names}
# review high school, middle school, and elementary school names
distinct(homes, hsdistrict)
distinct(homes, msdistrict)
distinct(homes, esdistrict)
```

### Your turn #2
Using the `homes` data, display the unique rows in the `condition` variable.
```{r your_turn 2}
distinct(homes, condition)
```

## Filter: filter rows
To return specific rows, use`filter()`.  When we use `filter()`, we can use various logical tests:

* ==, >, >=, <, <=, !=
* &, |
* x %in% y (x is a member of y)
* is.na(), !is.na()

```{r filter}
filter(homes, esdistrict == "Stone-Robinson") # Look at Stone-Robinson esdistrict homes only
filter(homes, yearbuilt > 2011) # homes built after 2011
filter(homes, condition %in% c("Poor", "Substandard")) 
```

### Your turn #3
Using the `homes` data, keep the observations where `finsqft` is less than 1000.  (Do not save the object.)
```{r your_turn 3}
filter(homes, finsqft < 1000)
```

### Filter for missings
Let's look at homes missing and not missing half-bath data.
```{r filter missing}
filter(homes, is.na(halfbath)) # homes missing halfbath data
filter(homes, !is.na(halfbath)) # homes not missing halfbath data
```

### Your turn #4
Using the `homes` data, keep the observations where `totalrooms` is missing.  (Do not save the object.)
```{r your_turn 4}
filter(homes, is.na(totalrooms))
```

### Value matching with %in%
The `%in%` value matching operator can be used to match conditions provided in a vector using the `c()` function.  You can think of `%in%` as **is a member of**.  It can replace multiple conditional statements linked together with OR (i.e., with a vertical bar `|`).

Let's focus on homes that are in the elementary school districts of two schools: Stone-Robinson and Cale.  We can do this with a vertical bar `|` which is the OR operator.
```{r vertical bar}
filter(homes, esdistrict == "Stone-Robinson" | esdistrict == "Cale") 
```

That's fine when we only have two matches, but it is easy to see how that could get tedious and lengthy if you have more statements you want to match.  That's where the value matching operator `%in%` comes in:

```{r filter by %in%}
filter(homes, esdistrict %in% c("Stone-Robinson", "Cale"))
```

## Your turn #5
Using the value matching operator, filter the `homes` dataset so only homes with `condition` of "Poor" or "Substandard" remain.  (Do not save the object.)
```{r your_turn 5}
filter(homes, condition %in% c("Poor", "Substandard"))
```

## Tip!
Remember: 

* `select()` returns specific columns
* `filter()` returns specific rows

## Arrange: sort rows
Use `arrange()` to sort the observations.  Let's take a closer look at which homes have the highest `totalvalue`.  
```{r arrange}
#look for max last sale price
arrange(homes, totalvalue)
```

Oops, we wanted the highest values at top.  Let's try the `desc()` helper function.
```{r arrange descending}
arrange(homes, desc(totalvalue))
```

### Your turn #6
Using the `homes` data, sort the observations so the smallest values for `finsqft` are on top.  (Do not save the object.)
```{r your_turn 6}
arrange(homes, finsqft)
```

## Count: number of observations
When working with data, we often want to know the number of observations found for each categorical variable. For this task, `dplyr` provides `count()`. Let's count the number of rows of data for each high school district:

```{r count}
count(homes, hsdistrict)
count(homes, hsdistrict, condition)
```

### Your turn #7
Use the `count()` function to create a table displaying the counts of `fullbath`.
```{r your_turn 7}
count(homes, fullbath)
```

## Pipe: a series of functions
What if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes.

With intermediate steps, you create a temporary data frame and use that as input to the next function, like this:
```{r intermediate steps}
monticello <- filter(homes, hsdistrict == "Monticello")
monticello_intermediate <- select(monticello, totalvalue, age)
```

This is readable, but can clutter up your workspace with lots of objects that you have to name individually. With multiple steps, that can be hard to keep track of.

You can also nest functions (i.e. one function inside of another), like this:
```{r nesting}
monticello_nest <- select(filter(homes, hsdistrict == "Monticello"), totalvalue, age)
```

This can be difficult to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, first `filter()`, then `select()`).

The best option is a "pipe." Pipes let you take the output of one function and send it directly to the next, which is useful when you want to apply many functions to the same data frame. The pipe operator looks like `%>%`.  You can also type *CTRL/CMD + Shift + M* as a keyboard shortcut.  (M since the pipe originally came from the `magrittr` package.)  It is helpful to say yourself "and then" when you see a pipe.

You don't need to put the data frame `homes` inside the parentheses.  It already gets piped in right at the beginning. A pipe moves left to right.

Let's recreate the above examples with a pipe:
```{r pipe}
monticello_pipe <- homes %>% 
  filter(hsdistrict == "Monticello") %>% 
  select(totalvalue, age)
```

What's happening here: 

* Take `homes` and then,
* Use `filter()` to retrieve only the observations that have hsdistrict equal to Monticello, and then
* Use `select()` to keep the `totalvalue` and `age` columns.

### Your turn #8
Using pipes, subset the `homes` data to include homes that are associated with the Western Albemarle high school district (`hsdistrict`), and retain only the columns `yearbuilt`, `totalrooms`, and `condition`.

Save to a new object: `west_alb`.
```{r your_turn 8}
west_alb <- homes %>%
  filter(hsdistrict == "Western Albemarle") %>% 
  select(yearbuilt, totalrooms, condition)
```

## Mutate: compute new variables 
Let's take a look at the summary statistics for `totalvalue`.  
```{r summary totalvalue}
summary(homes$totalvalue)
```

That max really is a lot higher than the other values.  We can use `dplyr::mutate()` to create, modify or delete numeric or logical columns.
```{r mutate}
# use mutate to log transform total value into a new numeric column and to compute new logical variable
homes <- homes %>% 
  mutate(logtotalvalue = log(totalvalue)) %>% 
  select(logtotalvalue, everything()) #puts new variable first
homes

```

Note that you have to remove missings for summary statistics.  You won't get an error, but the entire column will show up missing (NA).
```{r mutate mean with missings}
#you will get an error
homes <- homes %>% 
  mutate(pricier = lastsaleprice > mean(lastsaleprice)) %>% 
  select(pricier, everything())
homes
```

It works as expected if you remove the missings from the calculation of the mean:
```{r mutate mean}
homes <- homes %>% 
  mutate(pricier = lastsaleprice > mean(lastsaleprice, na.rm = TRUE)) %>% 
  select(pricier, everything())
homes
```

## Your turn #9
Create a new data frame from the `homes` data that meets the following criteria: contains only the `hsdistrict` column and a new column called `total_baths` containing a value that is equal to the total number of baths (`fullbath` plus `halfbath` times 0.5). Only the rows where total_baths is greater than 4.5 should be shown in the final data frame.

Hint: think about how the commands should be ordered to produce this data frame! 

Save object as: `homes_totalbaths`.
```{r your_turn 9}
homes_totalbaths <- homes %>% 
  mutate(total_baths = fullbath + halfbath * 0.5) %>% 
  filter(total_baths > 4.5) %>% 
  select(hsdistrict, total_baths)
homes_totalbaths
```

## Group_by and Summarize: compare groups with "split-apply-combine" data analysis
Many data analysis tasks can be approached using the split-apply-combine paradigm: 

* split the data into groups, 
* apply some analysis to each group, 
* and then combine the results as output. 

`dplyr` makes this very easy through the use of the `group_by()` and `summarize()` functions.  `group_by()` takes as arguments the column names that contain the *categorical* variables for which you want to calculate the summary statistics. `summarize()` collapses each group into a single-row summary of that group.

Let's compute the median total value by high school district:
```{r group_by and summarize}
homes %>% 
  group_by(hsdistrict) %>% 
  summarize(medtotalvalue = median(totalvalue, na.rm = TRUE))
```
Note that `summarize` and `summarise` are synonymous in dplyr.

You can also group by multiple variables.  Here we are grouping by high schools and by condition. Let's also sort the values so that the highest median total value is on top.
```{r group_by more than one attribute}
# group_by high school district and by condition
homes %>%
  group_by(hsdistrict, condition) %>%
  summarize(medtotalvalue = median(totalvalue, na.rm = TRUE),
            maxtotalvalue = max(totalvalue, na.rm = TRUE)) %>% 
  arrange(desc(medtotalvalue))

```

## Your turn #10
Use `group_by()` and `summarize()` to find the mean and max number of full baths for each middle school district.  (Do not save the object.)
```{r your_turn 10}
homes %>% 
  group_by(msdistrict) %>% 
  summarise(meanbath = mean(fullbath, na.rm = TRUE),
            maxbath = max(fullbath, na.rm = TRUE))
```

## Export your data
Similar to the `read_csv()` function we used to read in a csv file, there are write functions that creates files from a data frame.  This time let's use `write_rds()` to save to a native R data file. 

And since you did all that work, you might want to save a new, clean dataset.  Do not write over your raw data!

It's best practice to put your processed data in a folder separate from your raw data.

```{r export}
write_rds(homes, "../data-processed/albemarle_homes_2020_processed.Rds")
```

## Another quick R Markdown note
Now that we are finished, we can wrap things up by clicking the "Knit...Knit to HTML" button at the top.  We should see our complete document in the Viewer. 

*If you have never worked with R Markdown before, a "Install Required Packages" dialog will pop up that will ask if you want to install the required packages, which includes rmarkdown and knitr. Click Yes.*

## Cheat Sheets

* RStudio's [Data Wrangling with dplyr and tidyr Cheat sheet](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
* RStudio's [R Markdown Cheat Sheet](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)

## What we did not cover
There's only so much we can cover in a 90 minute workshop!  Here are more functions and packages to consider:

* `base::lapply()`: apply a function over a list or vector
* `base::union()`, `intersect()`, `setdiff()`, and `setequal()`: set operations
* `dplyr::bind_rows()` and `bind_cols()`: bind data frames by row and column
* `dplyr::left_join()`, `right_join()`, `inner_join()`, and `full_join()`: mutating joins on relational data
* `dplyr::semi_join()` and `anti_join()`: filtering joins on relational data
* `dplyr::mutate(across())`: applies the same transformation to multiple columns
* `tidyr::pivot_longer()` and `pivot_wider()` to reshape data
* `tidyr::unite()` and `separate()`: create new columns by pasting strings together and separate character columns with regular expressions
* `readr::parse_factors()`: parse factors
* `lubridate`: tidyverse package to work with dates and times
* `stringr`: tidyverse package to work with strings 
* `forcats`: tidyverse package for working with categorical variables
* `janitor::clean_names()`: clean column names, package offers more data cleaning functions

## More Learning Resources

Similar to what we covered today: 

* Data Carpentry's ["R for Social Scientists - Introducing dplyr and tidyr"](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html) - good for dplyr and tidyr
* R-Ladies Sydney's RYouWithMe [CleanItUp](https://rladiessydney.org/courses/ryouwithme/02-cleanitup-1/) series - good for dplyr and tidyr

Expanded coverage of data wrangling:

* R for Data Science [Wrangle](https://r4ds.had.co.nz/wrangle-intro.html) - covers more wrangling concepts than covered here

Previous UVA Library workshops on data wrangling using the Albemarle County homes data:

* [PhD Plus series](https://uvastatlab.github.io/phdplus2020/) for Exploratory Data Analysis and Data Preparation, by David Martin - step by step data prep for the Homes dataset we used today
* UVA Library Research Data Services Fall 2019 [Data Wrangling in R workshop](https://clayford.github.io/DWR/), by Clay Ford - wide coverage of data wrangling concepts

## Citation

This workshop was designed in part using Data Carpentry's ["R for Social Scientists - Introducing dplyr and tidyr"](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html) workshop, licensed under CC-BY 4.0 2018â€“2020 by The Carpentries, as well as R-Ladies Sydney RYouWithMe [CleanItUp](https://rladiessydney.org/courses/ryouwithme/02-cleanitup-1/) series.